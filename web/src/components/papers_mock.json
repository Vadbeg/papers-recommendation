[
  {
    "id": "automated-bridge-component-recognition-using",
    "title": "Automated Bridge Component Recognition using Video Data",
    "abstract": "This paper investigates the automated recognition of structural bridge\ncomponents using video data. Although understanding video data for structural\ninspections is straightforward for human inspectors, the implementation of the\nsame task using machine learning methods has not been fully realized. In\nparticular, single-frame image processing techniques, such as convolutional\nneural networks (CNNs), are not expected to identify structural components\naccurately when the image is a close-up view, lacking contextual information\nregarding where on the structure the image originates. Inspired by the\nsignificant progress in video processing techniques, this study investigates\nautomated bridge component recognition using video data, where the information\nfrom the past frames is used to augment the understanding of the current frame.\nA new simulated video dataset is created to train the machine learning\nalgorithms. Then, convolutional Neural Networks (CNNs) with recurrent\narchitectures are designed and applied to implement the automated bridge\ncomponent recognition task. Results are presented for simulated video data, as\nwell as video collected in the field."
  },
  {
    "id": "modularity-matters-learning-invariant",
    "title": "Modularity Matters: Learning Invariant Relational Reasoning Tasks",
    "abstract": "We focus on two supervised visual reasoning tasks whose labels encode a\nsemantic relational rule between two or more objects in an image: the MNIST\nParity task and the colorized Pentomino task. The objects in the images undergo\nrandom translation, scaling, rotation and coloring transformations. Thus these\ntasks involve invariant relational reasoning. We report uneven performance of\nvarious deep CNN models on these two tasks. For the MNIST Parity task, we\nreport that the VGG19 model soundly outperforms a family of ResNet models.\nMoreover, the family of ResNet models exhibits a general sensitivity to random\ninitialization for the MNIST Parity task. For the colorized Pentomino task, now\nboth the VGG19 and ResNet models exhibit sluggish optimization and very poor\ntest generalization, hovering around 30% test error. The CNN we tested all\nlearn hierarchies of fully distributed features and thus encode the distributed\nrepresentation prior. We are motivated by a hypothesis from cognitive\nneuroscience which posits that the human visual cortex is modularized, and this\nallows the visual cortex to learn higher order invariances. To this end, we\nconsider a modularized variant of the ResNet model, referred to as a Residual\nMixture Network (ResMixNet) which employs a mixture-of-experts architecture to\ninterleave distributed representations with more specialized, modular\nrepresentations. We show that very shallow ResMixNets are capable of learning\neach of the two tasks well, attaining less than 2% and 1% test error on the\nMNIST Parity and the colorized Pentomino tasks respectively. Most importantly,\nthe ResMixNet models are extremely parameter efficient: generalizing better\nthan various non-modular CNNs that have over 10x the number of parameters.\nThese experimental results support the hypothesis that modularity is a robust\nprior for learning invariant relational reasoning."
  },
  {
    "id": "an-ensemble-of-transfer-semi-supervised-and",
    "title": "An Ensemble of Transfer, Semi-supervised and Supervised Learning Methods for Pathological Heart Sound Classification",
    "abstract": "In this work, we propose an ensemble of classifiers to distinguish between\nvarious degrees of abnormalities of the heart using Phonocardiogram (PCG)\nsignals acquired using digital stethoscopes in a clinical setting, for the\nINTERSPEECH 2018 Computational Paralinguistics (ComParE) Heart Beats\nSubChallenge. Our primary classification framework constitutes a convolutional\nneural network with 1D-CNN time-convolution (tConv) layers, which uses features\ntransferred from a model trained on the 2016 Physionet Heart Sound Database. We\nalso employ a Representation Learning (RL) approach to generate features in an\nunsupervised manner using Deep Recurrent Autoencoders and use Support Vector\nMachine (SVM) and Linear Discriminant Analysis (LDA) classifiers. Finally, we\nutilize an SVM classifier on a high-dimensional segment-level feature extracted\nusing various functionals on short-term acoustic features, i.e., Low-Level\nDescriptors (LLD). An ensemble of the three different approaches provides a\nrelative improvement of 11.13% compared to our best single sub-system in terms\nof the Unweighted Average Recall (UAR) performance metric on the evaluation\ndataset."
  },
  {
    "id": "neural-style-transfer-a-review",
    "title": "Neural Style Transfer: A Review",
    "abstract": "The seminal work of Gatys et al. demonstrated the power of Convolutional\nNeural Networks (CNNs) in creating artistic imagery by separating and\nrecombining image content and style. This process of using CNNs to render a\ncontent image in different styles is referred to as Neural Style Transfer\n(NST). Since then, NST has become a trending topic both in academic literature\nand industrial applications. It is receiving increasing attention and a variety\nof approaches are proposed to either improve or extend the original NST\nalgorithm. In this paper, we aim to provide a comprehensive overview of the\ncurrent progress towards NST. We first propose a taxonomy of current algorithms\nin the field of NST. Then, we present several evaluation methods and compare\ndifferent NST algorithms both qualitatively and quantitatively. The review\nconcludes with a discussion of various applications of NST and open problems\nfor future research. A list of papers discussed in this review, corresponding\ncodes, pre-trained models and more comparison results are publicly available at\nhttps://github.com/ycjing/Neural-Style-Transfer-Papers."
  },
  {
    "id": "investigating-generative-adversarial-networks",
    "title": "Investigating Generative Adversarial Networks based Speech Dereverberation for Robust Speech Recognition",
    "abstract": "We investigate the use of generative adversarial networks (GANs) in speech\ndereverberation for robust speech recognition. GANs have been recently studied\nfor speech enhancement to remove additive noises, but there still lacks of a\nwork to examine their ability in speech dereverberation and the advantages of\nusing GANs have not been fully established. In this paper, we provide deep\ninvestigations in the use of GAN-based dereverberation front-end in ASR. First,\nwe study the effectiveness of different dereverberation networks (the generator\nin GAN) and find that LSTM leads a significant improvement as compared with\nfeed-forward DNN and CNN in our dataset. Second, further adding residual\nconnections in the deep LSTMs can boost the performance as well. Finally, we\nfind that, for the success of GAN, it is important to update the generator and\nthe discriminator using the same mini-batch data during training. Moreover,\nusing reverberant spectrogram as a condition to discriminator, as suggested in\nprevious studies, may degrade the performance. In summary, our GAN-based\ndereverberation front-end achieves 14%-19% relative CER reduction as compared\nto the baseline DNN dereverberation network when tested on a strong\nmulti-condition training acoustic model."
  },
  {
    "id": "learning-acoustic-word-embeddings-with-1",
    "title": "Learning Acoustic Word Embeddings with Temporal Context for Query-by-Example Speech Search",
    "abstract": "We propose to learn acoustic word embeddings with temporal context for\nquery-by-example (QbE) speech search. The temporal context includes the leading\nand trailing word sequences of a word. We assume that there exist spoken word\npairs in the training database. We pad the word pairs with their original\ntemporal context to form fixed-length speech segment pairs. We obtain the\nacoustic word embeddings through a deep convolutional neural network (CNN)\nwhich is trained on the speech segment pairs with a triplet loss. Shifting a\nfixed-length analysis window through the search content, we obtain a running\nsequence of embeddings. In this way, searching for the spoken query is\nequivalent to the matching of acoustic word embeddings. The experiments show\nthat our proposed acoustic word embeddings learned with temporal context are\neffective in QbE speech search. They outperform the state-of-the-art\nframe-level feature representations and reduce run-time computation since no\ndynamic time warping is required in QbE speech search. We also find that it is\nimportant to have sufficient speech segment pairs to train the deep CNN for\neffective acoustic word embeddings."
  },
  {
    "id": "personalized-saliency-and-its-prediction",
    "title": "Personalized Saliency and its Prediction",
    "abstract": "Nearly all existing visual saliency models by far have focused on predicting\na universal saliency map across all observers. Yet psychology studies suggest\nthat visual attention of different observers can vary significantly under\nspecific circumstances, especially a scene is composed of multiple salient\nobjects. To study such heterogenous visual attention pattern across observers,\nwe first construct a personalized saliency dataset and explore correlations\nbetween visual attention, personal preferences, and image contents.\nSpecifically, we propose to decompose a personalized saliency map (referred to\nas PSM) into a universal saliency map (referred to as USM) predictable by\nexisting saliency detection models and a new discrepancy map across users that\ncharacterizes personalized saliency. We then present two solutions towards\npredicting such discrepancy maps, i.e., a multi-task convolutional neural\nnetwork (CNN) framework and an extended CNN with Person-specific Information\nEncoded Filters (CNN-PIEF). Extensive experimental results demonstrate the\neffectiveness of our models for PSM prediction as well their generalization\ncapability for unseen observers."
  },
  {
    "id": "offline-extraction-of-indic-regional-language",
    "title": "Offline Extraction of Indic Regional Language from Natural Scene Image using Text Segmentation and Deep Convolutional Sequence",
    "abstract": "Regional language extraction from a natural scene image is always a\nchallenging proposition due to its dependence on the text information extracted\nfrom Image. Text Extraction on the other hand varies on different lighting\ncondition, arbitrary orientation, inadequate text information, heavy background\ninfluence over text and change of text appearance. This paper presents a novel\nunified method for tackling the above challenges. The proposed work uses an\nimage correction and segmentation technique on the existing Text Detection\nPipeline an Efficient and Accurate Scene Text Detector (EAST). EAST uses\nstandard PVAnet architecture to select features and non maximal suppression to\ndetect text from image. Text recognition is done using combined architecture of\nMaxOut convolution neural network (CNN) and Bidirectional long short term\nmemory (LSTM) network. After recognizing text using the Deep Learning based\napproach, the native Languages are translated to English and tokenized using\nstandard Text Tokenizers. The tokens that very likely represent a location is\nused to find the Global Positioning System (GPS) coordinates of the location\nand subsequently the regional languages spoken in that location is extracted.\nThe proposed method is tested on a self generated dataset collected from\nGovernment of India dataset and experimented on Standard Dataset to evaluate\nthe performance of the proposed technique. Comparative study with a few\nstate-of-the-art methods on text detection, recognition and extraction of\nregional language from images shows that the proposed method outperforms the\nexisting methods."
  },
  {
    "id": "accurate-spectral-super-resolution-from",
    "title": "Accurate Spectral Super-resolution from Single RGB Image Using Multi-scale CNN",
    "abstract": "Different from traditional hyperspectral super-resolution approaches that\nfocus on improving the spatial resolution, spectral super-resolution aims at\nproducing a high-resolution hyperspectral image from the RGB observation with\nsuper-resolution in spectral domain. However, it is challenging to accurately\nreconstruct a high-dimensional continuous spectrum from three discrete\nintensity values at each pixel, since too much information is lost during the\nprocedure where the latent hyperspectral image is downsampled (e.g., with x10\nscaling factor) in spectral domain to produce an RGB observation. To address\nthis problem, we present a multi-scale deep convolutional neural network (CNN)\nto explicitly map the input RGB image into a hyperspectral image. Through\nsymmetrically downsampling and upsampling the intermediate feature maps in a\ncascading paradigm, the local and non-local image information can be jointly\nencoded for spectral representation, ultimately improving the spectral\nreconstruction accuracy. Extensive experiments on a large hyperspectral dataset\ndemonstrate the effectiveness of the proposed method."
  },
  {
    "id": "semantic-video-segmentation-a-review-on",
    "title": "Semantic Video Segmentation: A Review on Recent Approaches",
    "abstract": "This paper gives an overview on semantic segmentation consists of an\nexplanation of this field, it's status and relation with other vision\nfundamental tasks, different datasets and common evaluation parameters that\nhave been used by researchers. This survey also includes an overall review on a\nvariety of recent approaches (RDF, MRF, CRF, etc.) and their advantages and\nchallenges and shows the superiority of CNN-based semantic segmentation systems\non CamVid and NYUDv2 datasets. In addition, some areas that is ideal for future\nwork have mentioned."
  },
  {
    "id": "real-time-deep-learning-method-for-abandoned",
    "title": "Real-Time Deep Learning Method for Abandoned Luggage Detection in Video",
    "abstract": "Recent terrorist attacks in major cities around the world have brought many\ncasualties among innocent citizens. One potential threat is represented by\nabandoned luggage items (that could contain bombs or biological warfare) in\npublic areas. In this paper, we describe an approach for real-time automatic\ndetection of abandoned luggage in video captured by surveillance cameras. The\napproach is comprised of two stages: (i) static object detection based on\nbackground subtraction and motion estimation and (ii) abandoned luggage\nrecognition based on a cascade of convolutional neural networks (CNN). To train\nour neural networks we provide two types of examples: images collected from the\nInternet and realistic examples generated by imposing various suitcases and\nbags over the scene's background. We present empirical results demonstrating\nthat our approach yields better performance than a strong CNN baseline method."
  },
  {
    "id": "optimizing-the-trade-off-between-single-stage",
    "title": "Optimizing the Trade-off between Single-Stage and Two-Stage Object Detectors using Image Difficulty Prediction",
    "abstract": "There are mainly two types of state-of-the-art object detectors. On one hand,\nwe have two-stage detectors, such as Faster R-CNN (Region-based Convolutional\nNeural Networks) or Mask R-CNN, that (i) use a Region Proposal Network to\ngenerate regions of interests in the first stage and (ii) send the region\nproposals down the pipeline for object classification and bounding-box\nregression. Such models reach the highest accuracy rates, but are typically\nslower. On the other hand, we have single-stage detectors, such as YOLO (You\nOnly Look Once) and SSD (Singe Shot MultiBox Detector), that treat object\ndetection as a simple regression problem by taking an input image and learning\nthe class probabilities and bounding box coordinates. Such models reach lower\naccuracy rates, but are much faster than two-stage object detectors. In this\npaper, we propose to use an image difficulty predictor to achieve an optimal\ntrade-off between accuracy and speed in object detection. The image difficulty\npredictor is applied on the test images to split them into easy versus hard\nimages. Once separated, the easy images are sent to the faster single-stage\ndetector, while the hard images are sent to the more accurate two-stage\ndetector. Our experiments on PASCAL VOC 2007 show that using image difficulty\ncompares favorably to a random split of the images. Our method is flexible, in\nthat it allows to choose a desired threshold for splitting the images into easy\nversus hard."
  },
  {
    "id": "dynamic-weight-alignment-for-temporal",
    "title": "Dynamic Weight Alignment for Temporal Convolutional Neural Networks",
    "abstract": "In this paper, we propose a method of improving temporal Convolutional Neural\nNetworks (CNN) by determining the optimal alignment of weights and inputs using\ndynamic programming. Conventional CNN convolutions linearly match the shared\nweights to a window of the input. However, it is possible that there exists a\nmore optimal alignment of weights. Thus, we propose the use of Dynamic Time\nWarping (DTW) to dynamically align the weights to the input of the\nconvolutional layer. Specifically, the dynamic alignment overcomes issues such\nas temporal distortion by finding the minimal distance matching of the weights\nand the inputs under constraints. We demonstrate the effectiveness of the\nproposed architecture on the Unipen online handwritten digit and character\ndatasets, the UCI Spoken Arabic Digit dataset, and the UCI Activities of Daily\nLife dataset."
  },
  {
    "id": "learning-front-end-filter-bank-parameters",
    "title": "Learning Front-end Filter-bank Parameters using Convolutional Neural Networks for Abnormal Heart Sound Detection",
    "abstract": "Automatic heart sound abnormality detection can play a vital role in the\nearly diagnosis of heart diseases, particularly in low-resource settings. The\nstate-of-the-art algorithms for this task utilize a set of Finite Impulse\nResponse (FIR) band-pass filters as a front-end followed by a Convolutional\nNeural Network (CNN) model. In this work, we propound a novel CNN architecture\nthat integrates the front-end bandpass filters within the network using\ntime-convolution (tConv) layers, which enables the FIR filter-bank parameters\nto become learnable. Different initialization strategies for the learnable\nfilters, including random parameters and a set of predefined FIR filter-bank\ncoefficients, are examined. Using the proposed tConv layers, we add constraints\nto the learnable FIR filters to ensure linear and zero phase responses.\nExperimental evaluations are performed on a balanced 4-fold cross-validation\ntask prepared using the PhysioNet/CinC 2016 dataset. Results demonstrate that\nthe proposed models yield superior performance compared to the state-of-the-art\nsystem, while the linear phase FIR filterbank method provides an absolute\nimprovement of 9.54% over the baseline in terms of an overall accuracy metric."
  },
  {
    "id": "hybrid-approach-of-relation-network-and",
    "title": "Hybrid Approach of Relation Network and Localized Graph Convolutional Filtering for Breast Cancer Subtype Classification",
    "abstract": "Network biology has been successfully used to help reveal complex mechanisms\nof disease, especially cancer. On the other hand, network biology requires\nin-depth knowledge to construct disease-specific networks, but our current\nknowledge is very limited even with the recent advances in human cancer\nbiology. Deep learning has shown a great potential to address the difficult\nsituation like this. However, deep learning technologies conventionally use\ngrid-like structured data, thus application of deep learning technologies to\nthe classification of human disease subtypes is yet to be explored. Recently,\ngraph based deep learning techniques have emerged, which becomes an opportunity\nto leverage analyses in network biology. In this paper, we proposed a hybrid\nmodel, which integrates two key components 1) graph convolution neural network\n(graph CNN) and 2) relation network (RN). We utilize graph CNN as a component\nto learn expression patterns of cooperative gene community, and RN as a\ncomponent to learn associations between learned patterns. The proposed model is\napplied to the PAM50 breast cancer subtype classification task, the standard\nbreast cancer subtype classification of clinical utility. In experiments of\nboth subtype classification and patient survival analysis, our proposed method\nachieved significantly better performances than existing methods. We believe\nthat this work is an important starting point to realize the upcoming\npersonalized medicine."
  },
  {
    "id": "learning-6-dof-grasping-interaction-via-deep",
    "title": "Learning 6-DOF Grasping Interaction via Deep Geometry-aware 3D Representations",
    "abstract": "This paper focuses on the problem of learning 6-DOF grasping with a parallel\njaw gripper in simulation. We propose the notion of a geometry-aware\nrepresentation in grasping based on the assumption that knowledge of 3D\ngeometry is at the heart of interaction. Our key idea is constraining and\nregularizing grasping interaction learning through 3D geometry prediction.\nSpecifically, we formulate the learning of deep geometry-aware grasping model\nin two steps: First, we learn to build mental geometry-aware representation by\nreconstructing the scene (i.e., 3D occupancy grid) from RGBD input via\ngenerative 3D shape modeling. Second, we learn to predict grasping outcome with\nits internal geometry-aware representation. The learned outcome prediction\nmodel is used to sequentially propose grasping solutions via\nanalysis-by-synthesis optimization. Our contributions are fourfold: (1) To best\nof our knowledge, we are presenting for the first time a method to learn a\n6-DOF grasping net from RGBD input; (2) We build a grasping dataset from\ndemonstrations in virtual reality with rich sensory and interaction\nannotations. This dataset includes 101 everyday objects spread across 7\ncategories, additionally, we propose a data augmentation strategy for effective\nlearning; (3) We demonstrate that the learned geometry-aware representation\nleads to about 10 percent relative performance improvement over the baseline\nCNN on grasping objects from our dataset. (4) We further demonstrate that the\nmodel generalizes to novel viewpoints and object instances."
  },
  {
    "id": "mobilefacenets-efficient-cnns-for-accurate",
    "title": "MobileFaceNets: Efficient CNNs for Accurate Real-Time Face Verification on Mobile Devices",
    "abstract": "Face Analysis Project on MXNet"
  },
  {
    "id": "gradient-descent-learns-one-hidden-layer-cnn",
    "title": "Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of Spurious Local Minima",
    "abstract": "We consider the problem of learning a one-hidden-layer neural network with\nnon-overlapping convolutional layer and ReLU activation, i.e., $f(\\mathbf{Z},\n\\mathbf{w}, \\mathbf{a}) = \\sum_j a_j\\sigma(\\mathbf{w}^T\\mathbf{Z}_j)$, in which\nboth the convolutional weights $\\mathbf{w}$ and the output weights $\\mathbf{a}$\nare parameters to be learned. When the labels are the outputs from a teacher\nnetwork of the same architecture with fixed weights $(\\mathbf{w}^*,\n\\mathbf{a}^*)$, we prove that with Gaussian input $\\mathbf{Z}$, there is a\nspurious local minimizer. Surprisingly, in the presence of the spurious local\nminimizer, gradient descent with weight normalization from randomly initialized\nweights can still be proven to recover the true parameters with constant\nprobability, which can be boosted to probability $1$ with multiple restarts. We\nalso show that with constant probability, the same procedure could also\nconverge to the spurious local minimum, showing that the local minimum plays a\nnon-trivial role in the dynamics of gradient descent. Furthermore, a\nquantitative analysis shows that the gradient descent dynamics has two phases:\nit starts off slow, but converges much faster after several iterations."
  },
  {
    "id": "insights-on-representational-similarity-in",
    "title": "Insights on representational similarity in neural networks with canonical correlation",
    "abstract": "Comparing different neural network representations and determining how\nrepresentations evolve over time remain challenging open questions in our\nunderstanding of the function of neural networks. Comparing representations in\nneural networks is fundamentally difficult as the structure of representations\nvaries greatly, even across groups of networks trained on identical tasks, and\nover the course of training. Here, we develop projection weighted CCA\n(Canonical Correlation Analysis) as a tool for understanding neural networks,\nbuilding off of SVCCA, a recently proposed method (Raghu et al., 2017). We\nfirst improve the core method, showing how to differentiate between signal and\nnoise, and then apply this technique to compare across a group of CNNs,\ndemonstrating that networks which generalize converge to more similar\nrepresentations than networks which memorize, that wider networks converge to\nmore similar solutions than narrow networks, and that trained networks with\nidentical topology but different learning rates converge to distinct clusters\nwith diverse representations. We also investigate the representational dynamics\nof RNNs, across both training and sequential timesteps, finding that RNNs\nconverge in a bottom-up pattern over the course of training and that the hidden\nstate is highly variable over the course of a sequence, even when accounting\nfor linear transforms. Together, these results provide new insights into the\nfunction of CNNs and RNNs, and demonstrate the utility of using CCA to\nunderstand representations."
  },
  {
    "id": "hgr-net-a-fusion-network-for-hand-gesture",
    "title": "HGR-Net: A Fusion Network for Hand Gesture Segmentation and Recognition",
    "abstract": "We propose a two-stage convolutional neural network (CNN) architecture for robust recognition of hand gestures, called HGR-Net, where the first stage performs accurate semantic segmentation to determine hand regions, and the second stage identifies the gesture. The segmentation stage architecture is based on the combination of fully convolutional residual network and atrous spatial pyramid pooling. Although the segmentation sub-network is trained without depth information, it is particularly robust against challenges such as illumination variations and complex backgrounds. The recognition stage deploys a two-stream CNN, which fuses the information from the red-green-blue and segmented images by combining their deep representations in a fully connected layer before classification. Extensive experiments on public datasets show that our architecture achieves almost as good as state-of-the-art performance in segmentation and recognition of static hand gestures, at a fraction of training time, run time, and model size. Our method can operate at an average of 23 ms per frame."
  },
  {
    "id": "voxceleb2-deep-speaker-recognition",
    "title": "VoxCeleb2: Deep Speaker Recognition",
    "abstract": "The objective of this paper is speaker recognition under noisy and\nunconstrained conditions.\n  We make two key contributions. First, we introduce a very large-scale\naudio-visual speaker recognition dataset collected from open-source media.\nUsing a fully automated pipeline, we curate VoxCeleb2 which contains over a\nmillion utterances from over 6,000 speakers. This is several times larger than\nany publicly available speaker recognition dataset.\n  Second, we develop and compare Convolutional Neural Network (CNN) models and\ntraining strategies that can effectively recognise identities from voice under\nvarious conditions. The models trained on the VoxCeleb2 dataset surpass the\nperformance of previous works on a benchmark dataset by a significant margin."
  },
  {
    "id": "real-time-cardiovascular-mr-with-spatio",
    "title": "Real-time Cardiovascular MR with Spatio-temporal Artifact Suppression using Deep Learning - Proof of Concept in Congenital Heart Disease",
    "abstract": "PURPOSE: Real-time assessment of ventricular volumes requires high\nacceleration factors. Residual convolutional neural networks (CNN) have shown\npotential for removing artifacts caused by data undersampling. In this study we\ninvestigated the effect of different radial sampling patterns on the accuracy\nof a CNN. We also acquired actual real-time undersampled radial data in\npatients with congenital heart disease (CHD), and compare CNN reconstruction to\nCompressed Sensing (CS).\n  METHODS: A 3D (2D plus time) CNN architecture was developed, and trained\nusing 2276 gold-standard paired 3D data sets, with 14x radial undersampling.\nFour sampling schemes were tested, using 169 previously unseen 3D 'synthetic'\ntest data sets. Actual real-time tiny Golden Angle (tGA) radial SSFP data was\nacquired in 10 new patients (122 3D data sets), and reconstructed using the 3D\nCNN as well as a CS algorithm; GRASP.\n  RESULTS: Sampling pattern was shown to be important for image quality, and\naccurate visualisation of cardiac structures. For actual real-time data,\noverall reconstruction time with CNN (including creation of aliased images) was\nshown to be more than 5x faster than GRASP. Additionally, CNN image quality and\naccuracy of biventricular volumes was observed to be superior to GRASP for the\nsame raw data.\n  CONCLUSION: This paper has demonstrated the potential for the use of a 3D CNN\nfor deep de-aliasing of real-time radial data, within the clinical setting.\nClinical measures of ventricular volumes using real-time data with CNN\nreconstruction are not statistically significantly different from the\ngold-standard, cardiac gated, BH techniques."
  },
  {
    "id": "entity-commonsense-representation-for-neural",
    "title": "Entity Commonsense Representation for Neural Abstractive Summarization",
    "abstract": "A major proportion of a text summary includes important entities found in the\noriginal text. These entities build up the topic of the summary. Moreover, they\nhold commonsense information once they are linked to a knowledge base. Based on\nthese observations, this paper investigates the usage of linked entities to\nguide the decoder of a neural text summarizer to generate concise and better\nsummaries. To this end, we leverage on an off-the-shelf entity linking system\n(ELS) to extract linked entities and propose Entity2Topic (E2T), a module\neasily attachable to a sequence-to-sequence model that transforms a list of\nentities into a vector representation of the topic of the summary. Current\navailable ELS's are still not sufficiently effective, possibly introducing\nunresolved ambiguities and irrelevant entities. We resolve the imperfections of\nthe ELS by (a) encoding entities with selective disambiguation, and (b) pooling\nentity vectors using firm attention. By applying E2T to a simple\nsequence-to-sequence model with attention mechanism as base model, we see\nsignificant improvements of the performance in the Gigaword (sentence to title)\nand CNN (long document to multi-sentence highlights) summarization datasets by\nat least 2 ROUGE points."
  },
  {
    "id": "copycat-cnn-stealing-knowledge-by-persuading",
    "title": "Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data",
    "abstract": "In the past few years, Convolutional Neural Networks (CNNs) have been\nachieving state-of-the-art performance on a variety of problems. Many companies\nemploy resources and money to generate these models and provide them as an API,\ntherefore it is in their best interest to protect them, i.e., to avoid that\nsomeone else copies them. Recent studies revealed that state-of-the-art CNNs\nare vulnerable to adversarial examples attacks, and this weakness indicates\nthat CNNs do not need to operate in the problem domain (PD). Therefore, we\nhypothesize that they also do not need to be trained with examples of the PD in\norder to operate in it.\n  Given these facts, in this paper, we investigate if a target black-box CNN\ncan be copied by persuading it to confess its knowledge through random\nnon-labeled data. The copy is two-fold: i) the target network is queried with\nrandom data and its predictions are used to create a fake dataset with the\nknowledge of the network; and ii) a copycat network is trained with the fake\ndataset and should be able to achieve similar performance as the target\nnetwork.\n  This hypothesis was evaluated locally in three problems (facial expression,\nobject, and crosswalk classification) and against a cloud-based API. In the\ncopy attacks, images from both non-problem domain and PD were used. All copycat\nnetworks achieved at least 93.7% of the performance of the original models with\nnon-problem domain data, and at least 98.6% using additional data from the PD.\nAdditionally, the copycat CNN successfully copied at least 97.3% of the\nperformance of the Microsoft Azure Emotion API. Our results show that it is\npossible to create a copycat CNN by simply querying a target network as\nblack-box with random non-labeled data."
  },
  {
    "id": "relation-networks-for-object-detection",
    "title": "Relation Networks for Object Detection",
    "abstract": "Although it is well believed for years that modeling relations between\nobjects would help object recognition, there has not been evidence that the\nidea is working in the deep learning era. All state-of-the-art object detection\nsystems still rely on recognizing object instances individually, without\nexploiting their relations during learning.\n  This work proposes an object relation module. It processes a set of objects\nsimultaneously through interaction between their appearance feature and\ngeometry, thus allowing modeling of their relations. It is lightweight and\nin-place. It does not require additional supervision and is easy to embed in\nexisting networks. It is shown effective on improving object recognition and\nduplicate removal steps in the modern object detection pipeline. It verifies\nthe efficacy of modeling object relations in CNN based detection. It gives rise\nto the first fully end-to-end object detector."
  },
  {
    "id": "dynamical-isometry-and-a-mean-field-theory-of-2",
    "title": "Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks",
    "abstract": "In recent years, state-of-the-art methods in computer vision have utilized\nincreasingly deep convolutional neural network architectures (CNNs), with some\nof the most successful models employing hundreds or even thousands of layers. A\nvariety of pathologies such as vanishing/exploding gradients make training such\ndeep networks challenging. While residual connections and batch normalization\ndo enable training at these depths, it has remained unclear whether such\nspecialized architecture designs are truly necessary to train deep CNNs. In\nthis work, we demonstrate that it is possible to train vanilla CNNs with ten\nthousand layers or more simply by using an appropriate initialization scheme.\nWe derive this initialization scheme theoretically by developing a mean field\ntheory for signal propagation and by characterizing the conditions for\ndynamical isometry, the equilibration of singular values of the input-output\nJacobian matrix. These conditions require that the convolution operator be an\northogonal transformation in the sense that it is norm-preserving. We present\nan algorithm for generating such random initial orthogonal convolution kernels\nand demonstrate empirically that they enable efficient training of extremely\ndeep architectures."
  },
  {
    "id": "multi-attention-multi-class-constraint-for",
    "title": "Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition",
    "abstract": "Attention-based learning for fine-grained image recognition remains a\nchallenging task, where most of the existing methods treat each object part in\nisolation, while neglecting the correlations among them. In addition, the\nmulti-stage or multi-scale mechanisms involved make the existing methods less\nefficient and hard to be trained end-to-end. In this paper, we propose a novel\nattention-based convolutional neural network (CNN) which regulates multiple\nobject parts among different input images. Our method first learns multiple\nattention region features of each input image through the one-squeeze\nmulti-excitation (OSME) module, and then apply the multi-attention multi-class\nconstraint (MAMC) in a metric learning framework. For each anchor feature, the\nMAMC functions by pulling same-attention same-class features closer, while\npushing different-attention or different-class features away. Our method can be\neasily trained end-to-end, and is highly efficient which requires only one\ntraining stage. Moreover, we introduce Dogs-in-the-Wild, a comprehensive dog\nspecies dataset that surpasses similar existing datasets by category coverage,\ndata volume and annotation quality. This dataset will be released upon\nacceptance to facilitate the research of fine-grained image recognition.\nExtensive experiments are conducted to show the substantial improvements of our\nmethod on four benchmark datasets."
  },
  {
    "id": "view-volume-network-for-semantic-scene",
    "title": "View-volume Network for Semantic Scene Completion from a Single Depth Image",
    "abstract": "We introduce a View-Volume convolutional neural network (VVNet) for inferring\nthe occupancy and semantic labels of a volumetric 3D scene from a single depth\nimage. The VVNet concatenates a 2D view CNN and a 3D volume CNN with a\ndifferentiable projection layer. Given a single RGBD image, our method extracts\nthe detailed geometric features from the input depth image with a 2D view CNN\nand then projects the features into a 3D volume according to the input depth\nmap via a projection layer. After that, we learn the 3D context information of\nthe scene with a 3D volume CNN for computing the result volumetric occupancy\nand semantic labels. With combined 2D and 3D representations, the VVNet\nefficiently reduces the computational cost, enables feature extraction from\nmulti-channel high resolution inputs, and thus significantly improves the\nresult accuracy. We validate our method and demonstrate its efficiency and\neffectiveness on both synthetic SUNCG and real NYU dataset."
  },
  {
    "id": "stingray-detection-of-aerial-images-using",
    "title": "Stingray Detection of Aerial Images Using Augmented Training Images Generated by A Conditional Generative Model",
    "abstract": "In this paper, we present an object detection method that tackles the\nstingray detection problem based on aerial images. In this problem, the images\nare aerially captured on a sea-surface area by using an Unmanned Aerial Vehicle\n(UAV), and the stingrays swimming under (but close to) the sea surface are the\ntarget we want to detect and locate. To this end, we use a deep object\ndetection method, faster RCNN, to train a stingray detector based on a limited\ntraining set of images. To boost the performance, we develop a new generative\napproach, conditional GLO, to increase the training samples of stingray, which\nis an extension of the Generative Latent Optimization (GLO) approach. Unlike\ntraditional data augmentation methods that generate new data only for image\nclassification, our proposed method that mixes foreground and background\ntogether can generate new data for an object detection task, and thus improve\nthe training efficacy of a CNN detector. Experimental results show that\nsatisfiable performance can be obtained by using our approach on stingray\ndetection in aerial images."
  },
  {
    "id": "scsp-spectral-clustering-filter-pruning-with",
    "title": "SCSP: Spectral Clustering Filter Pruning with Soft Self-adaption Manners",
    "abstract": "Deep Convolutional Neural Networks (CNN) has achieved significant success in\ncomputer vision field. However, the high computational cost of the deep complex\nmodels prevents the deployment on edge devices with limited memory and\ncomputational resource. In this paper, we proposed a novel filter pruning for\nconvolutional neural networks compression, namely spectral clustering filter\npruning with soft self-adaption manners (SCSP). We first apply spectral\nclustering on filters layer by layer to explore their intrinsic connections and\nonly count on efficient groups. By self-adaption manners, the pruning\noperations can be done in few epochs to let the network gradually choose\nmeaningful groups. According to this strategy, we not only achieve model\ncompression while keeping considerable performance, but also find a novel angle\nto interpret the model compression process."
  },
  {
    "id": "end-to-end-parkinson-disease-diagnosis-using",
    "title": "End-to-End Parkinson Disease Diagnosis using Brain MR-Images by 3D-CNN",
    "abstract": "In this work, we use a deep learning framework for simultaneous\nclassification and regression of Parkinson disease diagnosis based on MR-Images\nand personal information (i.e. age, gender). We intend to facilitate and\nincrease the confidence in Parkinson disease diagnosis through our deep\nlearning framework."
  },
  {
    "id": "on-tighter-generalization-bound-for-deep",
    "title": "On Tighter Generalization Bound for Deep Neural Networks: CNNs, ResNets, and Beyond",
    "abstract": "We establish a margin based data dependent generalization error bound for a general family of deep neural networks in terms of the depth and width, as well as the Jacobian of the networks. Through introducing a new characterization of the Lipschitz properties of neural network family, we achieve significantly tighter generalization bounds than existing results. Moreover, we show that the generalization bound can be further improved for bounded losses. Aside from the general feedforward deep neural networks, our results can be applied to derive new bounds for popular architectures, including convolutional neural networks (CNNs) and residual networks (ResNets). When achieving same generalization errors with previous arts, our bounds allow for the choice of larger parameter spaces of weight matrices, inducing potentially stronger expressive ability for neural networks. Numerical evaluation is also provided to support our theory."
  },
  {
    "id": "automated-performance-assessment-in",
    "title": "Automated Performance Assessment in Transoesophageal Echocardiography with Convolutional Neural Networks",
    "abstract": "Transoesophageal echocardiography (TEE) is a valuable diagnostic and\nmonitoring imaging modality. Proper image acquisition is essential for\ndiagnosis, yet current assessment techniques are solely based on manual expert\nreview. This paper presents a supervised deep learn ing framework for\nautomatically evaluating and grading the quality of TEE images. To obtain the\nnecessary dataset, 38 participants of varied experience performed TEE exams\nwith a high-fidelity virtual reality (VR) platform. Two Convolutional Neural\nNetwork (CNN) architectures, AlexNet and VGG, structured to perform regression,\nwere finetuned and validated on manually graded images from three evaluators.\nTwo different scoring strategies, a criteria-based percentage and an overall\ngeneral impression, were used. The developed CNN models estimate the average\nscore with a root mean square accuracy ranging between 84%-93%, indicating the\nability to replicate expert valuation. Proposed strategies for automated TEE\nassessment can have a significant impact on the training process of new TEE\noperators, providing direct feedback and facilitating the development of the\nnecessary dexterous skills."
  },
  {
    "id": "3d-convolutional-neural-networks-for",
    "title": "3D Convolutional Neural Networks for Classification of Functional Connectomes",
    "abstract": "Resting-state functional MRI (rs-fMRI) scans hold the potential to serve as a\ndiagnostic or prognostic tool for a wide variety of conditions, such as autism,\nAlzheimer's disease, and stroke. While a growing number of studies have\ndemonstrated the promise of machine learning algorithms for rs-fMRI based\nclinical or behavioral prediction, most prior models have been limited in their\ncapacity to exploit the richness of the data. For example, classification\ntechniques applied to rs-fMRI often rely on region-based summary statistics\nand/or linear models. In this work, we propose a novel volumetric Convolutional\nNeural Network (CNN) framework that takes advantage of the full-resolution 3D\nspatial structure of rs-fMRI data and fits non-linear predictive models. We\nshowcase our approach on a challenging large-scale dataset (ABIDE, with N >\n2,000) and report state-of-the-art accuracy results on rs-fMRI-based\ndiscrimination of autism patients and healthy controls."
  },
  {
    "id": "exploiting-inherent-error-resiliency-of",
    "title": "Exploiting Inherent Error-Resiliency of Neuromorphic Computing to achieve Extreme Energy-Efficiency through Mixed-Signal Neurons",
    "abstract": "Neuromorphic computing, inspired by the brain, promises extreme efficiency\nfor certain classes of learning tasks, such as classification and pattern\nrecognition. The performance and power consumption of neuromorphic computing\ndepends heavily on the choice of the neuron architecture. Digital neurons\n(Dig-N) are conventionally known to be accurate and efficient at high speed,\nwhile suffering from high leakage currents from a large number of transistors\nin a large design. On the other hand, analog/mixed-signal neurons are prone to\nnoise, variability and mismatch, but can lead to extremely low-power designs.\nIn this work, we will analyze, compare and contrast existing neuron\narchitectures with a proposed mixed-signal neuron (MS-N) in terms of\nperformance, power and noise, thereby demonstrating the applicability of the\nproposed mixed-signal neuron for achieving extreme energy-efficiency in\nneuromorphic computing. The proposed MS-N is implemented in 65 nm CMOS\ntechnology and exhibits > 100X better energy-efficiency across all frequencies\nover two traditional digital neurons synthesized in the same technology node.\nWe also demonstrate that the inherent error-resiliency of a fully connected or\neven convolutional neural network (CNN) can handle the noise as well as the\nmanufacturing non-idealities of the MS-N up to certain degrees. Notably, a\nsystem-level implementation on MNIST datasets exhibits a worst-case increase in\nclassification error by 2.1% when the integrated noise power in the bandwidth\nis ~ 0.1 uV2, along with +-3{\\sigma} amount of variation and mismatch\nintroduced in the transistor parameters for the proposed neuron with 8-bit\nprecision."
  },
  {
    "id": "multiple-instance-learning-for-heterogeneous",
    "title": "Multiple Instance Learning for Heterogeneous Images: Training a CNN for Histopathology",
    "abstract": "Multiple instance (MI) learning with a convolutional neural network enables\nend-to-end training in the presence of weak image-level labels. We propose a\nnew method for aggregating predictions from smaller regions of the image into\nan image-level classification by using the quantile function. The quantile\nfunction provides a more complete description of the heterogeneity within each\nimage, improving image-level classification. We also adapt image augmentation\nto the MI framework by randomly selecting cropped regions on which to apply MI\naggregation during each epoch of training. This provides a mechanism to study\nthe importance of MI learning. We validate our method on five different\nclassification tasks for breast tumor histology and provide a visualization\nmethod for interpreting local image classifications that could lead to future\ninsights into tumor heterogeneity."
  },
  {
    "id": "beyond-counting-comparisons-of-density-maps",
    "title": "Beyond Counting: Comparisons of Density Maps for Crowd Analysis Tasks - Counting, Detection, and Tracking",
    "abstract": "For crowded scenes, the accuracy of object-based computer vision methods\ndeclines when the images are low-resolution and objects have severe occlusions.\nTaking counting methods for example, almost all the recent state-of-the-art\ncounting methods bypass explicit detection and adopt regression-based methods\nto directly count the objects of interest. Among regression-based methods,\ndensity map estimation, where the number of objects inside a subregion is the\nintegral of the density map over that subregion, is especially promising\nbecause it preserves spatial information, which makes it useful for both\ncounting and localization (detection and tracking). With the power of deep\nconvolutional neural networks (CNNs) the counting performance has improved\nsteadily. The goal of this paper is to evaluate density maps generated by\ndensity estimation methods on a variety of crowd analysis tasks, including\ncounting, detection, and tracking. Most existing CNN methods produce density\nmaps with resolution that is smaller than the original images, due to the\ndownsample strides in the convolution/pooling operations. To produce an\noriginal-resolution density map, we also evaluate a classical CNN that uses a\nsliding window regressor to predict the density for every pixel in the image.\nWe also consider a fully convolutional (FCNN) adaptation, with skip connections\nfrom lower convolutional layers to compensate for loss in spatial information\nduring upsampling. In our experiments, we found that the lower-resolution\ndensity maps sometimes have better counting performance. In contrast, the\noriginal-resolution density maps improved localization tasks, such as detection\nand tracking, compared to bilinear upsampling the lower-resolution density\nmaps. Finally, we also propose several metrics for measuring the quality of a\ndensity map, and relate them to experiment results on counting and\nlocalization."
  },
  {
    "id": "hyperdrive-a-systolically-scalable-binary",
    "title": "Hyperdrive: A Multi-Chip Systolically Scalable Binary-Weight CNN Inference Engine",
    "abstract": "Deep neural networks have achieved impressive results in computer vision and\nmachine learning. Unfortunately, state-of-the-art networks are extremely\ncompute and memory intensive which makes them unsuitable for mW-devices such as\nIoT end-nodes. Aggressive quantization of these networks dramatically reduces\nthe computation and memory footprint. Binary-weight neural networks (BWNs)\nfollow this trend, pushing weight quantization to the limit. Hardware\naccelerators for BWNs presented up to now have focused on core efficiency,\ndisregarding I/O bandwidth and system-level efficiency that are crucial for\ndeployment of accelerators in ultra-low power devices. We present Hyperdrive: a\nBWN accelerator dramatically reducing the I/O bandwidth exploiting a novel\nbinary-weight streaming approach, which can be used for arbitrarily sized\nconvolutional neural network architecture and input resolution by exploiting\nthe natural scalability of the compute units both at chip-level and\nsystem-level by arranging Hyperdrive chips systolically in a 2D mesh while\nprocessing the entire feature map together in parallel. Hyperdrive achieves 4.3\nTOp/s/W system-level efficiency (i.e., including I/Os)---3.1x higher than\nstate-of-the-art BWN accelerators, even if its core uses resource-intensive\nFP16 arithmetic for increased robustness."
  },
  {
    "id": "double-path-networks-for-sequence-to-sequence",
    "title": "Double Path Networks for Sequence to Sequence Learning",
    "abstract": "Encoder-decoder based Sequence to Sequence learning (S2S) has made remarkable\nprogress in recent years. Different network architectures have been used in the\nencoder/decoder. Among them, Convolutional Neural Networks (CNN) and Self\nAttention Networks (SAN) are the prominent ones. The two architectures achieve\nsimilar performances but use very different ways to encode and decode context:\nCNN use convolutional layers to focus on the local connectivity of the\nsequence, while SAN uses self-attention layers to focus on global semantics. In\nthis work we propose Double Path Networks for Sequence to Sequence learning\n(DPN-S2S), which leverage the advantages of both models by using double path\ninformation fusion. During the encoding step, we develop a double path\narchitecture to maintain the information coming from different paths with\nconvolutional layers and self-attention layers separately. To effectively use\nthe encoded context, we develop a cross attention module with gating and use it\nto automatically pick up the information needed during the decoding step. By\ndeeply integrating the two paths with cross attention, both types of\ninformation are combined and well exploited. Experiments show that our proposed\nmethod can significantly improve the performance of sequence to sequence\nlearning over state-of-the-art systems."
  },
  {
    "id": "3d-pose-estimation-for-fine-grained-object",
    "title": "3D Pose Estimation for Fine-Grained Object Categories",
    "abstract": "Existing object pose estimation datasets are related to generic object types\nand there is so far no dataset for fine-grained object categories. In this\nwork, we introduce a new large dataset to benchmark pose estimation for\nfine-grained objects, thanks to the availability of both 2D and 3D fine-grained\ndata recently. Specifically, we augment two popular fine-grained recognition\ndatasets (StanfordCars and CompCars) by finding a fine-grained 3D CAD model for\neach sub-category and manually annotating each object in images with 3D pose.\nWe show that, with enough training data, a full perspective model with\ncontinuous parameters can be estimated using 2D appearance information alone.\nWe achieve this via a framework based on Faster/Mask R-CNN. This goes beyond\nprevious works on category-level pose estimation, which only estimate\ndiscrete/continuous viewpoint angles or recover rotation matrices often with\nthe help of key points. Furthermore, with fine-grained 3D models available, we\nincorporate a dense 3D representation named as location field into the\nCNN-based pose estimation framework to further improve the performance. The new\ndataset is available at www.umiacs.umd.edu/~wym/3dpose.html"
  },
  {
    "id": "deep-sparse-coding-for-invariant-multimodal",
    "title": "Deep Sparse Coding for Invariant Multimodal Halle Berry Neurons",
    "abstract": "Deep feed-forward convolutional neural networks (CNNs) have become ubiquitous\nin virtually all machine learning and computer vision challenges; however,\nadvancements in CNNs have arguably reached an engineering saturation point\nwhere incremental novelty results in minor performance gains. Although there is\nevidence that object classification has reached human levels on narrowly\ndefined tasks, for general applications, the biological visual system is far\nsuperior to that of any computer. Research reveals there are numerous missing\ncomponents in feed-forward deep neural networks that are critical in mammalian\nvision. The brain does not work solely in a feed-forward fashion, but rather\nall of the neurons are in competition with each other; neurons are integrating\ninformation in a bottom up and top down fashion and incorporating expectation\nand feedback in the modeling process. Furthermore, our visual cortex is working\nin tandem with our parietal lobe, integrating sensory information from various\nmodalities.\n  In our work, we sought to improve upon the standard feed-forward deep\nlearning model by augmenting them with biologically inspired concepts of\nsparsity, top-down feedback, and lateral inhibition. We define our model as a\nsparse coding problem using hierarchical layers. We solve the sparse coding\nproblem with an additional top-down feedback error driving the dynamics of the\nneural network. While building and observing the behavior of our model, we were\nfascinated that multimodal, invariant neurons naturally emerged that mimicked,\n\"Halle Berry neurons\" found in the human brain. Furthermore, our sparse\nrepresentation of multimodal signals demonstrates qualitative and quantitative\nsuperiority to the standard feed-forward joint embedding in common vision and\nmachine learning tasks."
  },
  {
    "id": "direct-estimation-of-pharmacokinetic",
    "title": "Direct Estimation of Pharmacokinetic Parameters from DCE-MRI using Deep CNN with Forward Physical Model Loss",
    "abstract": "Dynamic contrast-enhanced (DCE) MRI is an evolving imaging technique that\nprovides a quantitative measure of pharmacokinetic (PK) parameters in body\ntissues, in which series of T1-weighted images are collected following the\nadministration of a paramagnetic contrast agent. Unfortunately, in many\napplications, conventional clinical DCE-MRI suffers from low spatiotemporal\nresolution and insufficient volume coverage. In this paper, we propose a novel\ndeep learning based approach to directly estimate the PK parameters from\nundersampled DCE-MRI data. Specifically, we design a custom loss function where\nwe incorporate a forward physical model that relates the PK parameters to\ncorrupted image-time series obtained due to subsampling in k-space. This allows\nthe network to directly exploit the knowledge of true contrast agent kinetics\nin the training phase, and hence provide more accurate restoration of PK\nparameters. Experiments on clinical brain DCE datasets demonstrate the efficacy\nof our approach in terms of fidelity of PK parameter reconstruction and\nsignificantly faster parameter inference compared to a model-based iterative\nreconstruction method."
  },
  {
    "id": "sample-dropout-for-audio-scene-classification",
    "title": "Sample Dropout for Audio Scene Classification Using Multi-Scale Dense Connected Convolutional Neural Network",
    "abstract": "Acoustic scene classification is an intricate problem for a machine. As an\nemerging field of research, deep Convolutional Neural Networks (CNN) achieve\nconvincing results. In this paper, we explore the use of multi-scale Dense\nconnected convolutional neural network (DenseNet) for the classification task,\nwith the goal to improve the classification performance as multi-scale features\ncan be extracted from the time-frequency representation of the audio signal. On\nthe other hand, most of previous CNN-based audio scene classification\napproaches aim to improve the classification accuracy, by employing different\nregularization techniques, such as the dropout of hidden units and data\naugmentation, to reduce overfitting. It is widely known that outliers in the\ntraining set have a high negative influence on the trained model, and culling\nthe outliers may improve the classification performance, while it is often\nunder-explored in previous studies. In this paper, inspired by the silence\nremoval in the speech signal processing, a novel sample dropout approach is\nproposed, which aims to remove outliers in the training dataset. Using the\nDCASE 2017 audio scene classification datasets, the experimental results\ndemonstrates the proposed multi-scale DenseNet providing a superior performance\nthan the traditional single-scale DenseNet, while the sample dropout method can\nfurther improve the classification robustness of multi-scale DenseNet."
  },
  {
    "id": "learning-a-discriminative-filter-bank-within",
    "title": "Learning a Discriminative Filter Bank within a CNN for Fine-grained Recognition",
    "abstract": "Compared to earlier multistage frameworks using CNN features, recent\nend-to-end deep approaches for fine-grained recognition essentially enhance the\nmid-level learning capability of CNNs. Previous approaches achieve this by\nintroducing an auxiliary network to infuse localization information into the\nmain classification network, or a sophisticated feature encoding method to\ncapture higher order feature statistics. We show that mid-level representation\nlearning can be enhanced within the CNN framework, by learning a bank of\nconvolutional filters that capture class-specific discriminative patches\nwithout extra part or bounding box annotations. Such a filter bank is well\nstructured, properly initialized and discriminatively learned through a novel\nasymmetric multi-stream architecture with convolutional filter supervision and\na non-random layer initialization. Experimental results show that our approach\nachieves state-of-the-art on three publicly available fine-grained recognition\ndatasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and\nvisualizations are provided to understand our approach."
  },
  {
    "id": "dpatch-an-adversarial-patch-attack-on-object",
    "title": "DPatch: An Adversarial Patch Attack on Object Detectors",
    "abstract": "Object detectors have emerged as an indispensable module in modern computer\nvision systems. In this work, we propose DPatch -- a black-box\nadversarial-patch-based attack towards mainstream object detectors (i.e. Faster\nR-CNN and YOLO). Unlike the original adversarial patch that only manipulates\nimage-level classifier, our DPatch simultaneously attacks the bounding box\nregression and object classification so as to disable their predictions.\nCompared to prior works, DPatch has several appealing properties: (1) DPatch\ncan perform both untargeted and targeted effective attacks, degrading the mAP\nof Faster R-CNN and YOLO from 75.10% and 65.7% down to below 1%, respectively.\n(2) DPatch is small in size and its attacking effect is location-independent,\nmaking it very practical to implement real-world attacks. (3) DPatch\ndemonstrates great transferability among different detectors as well as\ntraining datasets. For example, DPatch that is trained on Faster R-CNN can\neffectively attack YOLO, and vice versa. Extensive evaluations imply that\nDPatch can perform effective attacks under black-box setup, i.e., even without\nthe knowledge of the attacked network's architectures and parameters.\nSuccessful realization of DPatch also illustrates the intrinsic vulnerability\nof the modern detector architectures to such patch-based adversarial attacks."
  },
  {
    "id": "group-normalization",
    "title": "Group Normalization",
    "abstract": "FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet."
  },
  {
    "id": "physical-representation-based-predicate",
    "title": "Physical Representation-based Predicate Optimization for a Visual Analytics Database",
    "abstract": "Querying the content of images, video, and other non-textual data sources\nrequires expensive content extraction methods. Modern extraction techniques are\nbased on deep convolutional neural networks (CNNs) and can classify objects\nwithin images with astounding accuracy. Unfortunately, these methods are slow:\nprocessing a single image can take about 10 milliseconds on modern GPU-based\nhardware. As massive video libraries become ubiquitous, running a content-based\nquery over millions of video frames is prohibitive.\n  One promising approach to reduce the runtime cost of queries of visual\ncontent is to use a hierarchical model, such as a cascade, where simple cases\nare handled by an inexpensive classifier. Prior work has sought to design\ncascades that optimize the computational cost of inference by, for example,\nusing smaller CNNs. However, we observe that there are critical factors besides\nthe inference time that dramatically impact the overall query time. Notably, by\ntreating the physical representation of the input image as part of our query\noptimization---that is, by including image transforms, such as resolution\nscaling or color-depth reduction, within the cascade---we can optimize data\nhandling costs and enable drastically more efficient classifier cascades.\n  In this paper, we propose Tahoma, which generates and evaluates many\npotential classifier cascades that jointly optimize the CNN architecture and\ninput data representation. Our experiments on a subset of ImageNet show that\nTahoma's input transformations speed up cascades by up to 35 times. We also\nfind up to a 98x speedup over the ResNet50 classifier with no loss in accuracy,\nand a 280x speedup if some accuracy is sacrificed."
  },
  {
    "id": "collaborative-human-ai-chai-evidence-based",
    "title": "Collaborative Human-AI (CHAI): Evidence-Based Interpretable Melanoma Classification in Dermoscopic Images",
    "abstract": "Automated dermoscopic image analysis has witnessed rapid growth in diagnostic\nperformance. Yet adoption faces resistance, in part, because no evidence is\nprovided to support decisions. In this work, an approach for evidence-based\nclassification is presented. A feature embedding is learned with CNNs,\ntriplet-loss, and global average pooling, and used to classify via kNN search.\nEvidence is provided as both the discovered neighbors, as well as localized\nimage regions most relevant to measuring distance between query and neighbors.\nTo ensure that results are relevant in terms of both label accuracy and human\nvisual similarity for any skill level, a novel hierarchical triplet logic is\nimplemented to jointly learn an embedding according to disease labels and\nnon-expert similarity. Results are improved over baselines trained on disease\nlabels alone, as well as standard multiclass loss. Quantitative relevance of\nresults, according to non-expert similarity, as well as localized image\nregions, are also significantly improved."
  },
  {
    "id": "roto-translation-covariant-convolutional",
    "title": "Roto-Translation Covariant Convolutional Networks for Medical Image Analysis",
    "abstract": "We propose a framework for rotation and translation covariant deep learning\nusing $SE(2)$ group convolutions. The group product of the special Euclidean\nmotion group $SE(2)$ describes how a concatenation of two roto-translations\nresults in a net roto-translation. We encode this geometric structure into\nconvolutional neural networks (CNNs) via $SE(2)$ group convolutional layers,\nwhich fit into the standard 2D CNN framework, and which allow to generically\ndeal with rotated input samples without the need for data augmentation.\n  We introduce three layers: a lifting layer which lifts a 2D (vector valued)\nimage to an $SE(2)$-image, i.e., 3D (vector valued) data whose domain is\n$SE(2)$; a group convolution layer from and to an $SE(2)$-image; and a\nprojection layer from an $SE(2)$-image to a 2D image. The lifting and group\nconvolution layers are $SE(2)$ covariant (the output roto-translates with the\ninput). The final projection layer, a maximum intensity projection over\nrotations, makes the full CNN rotation invariant.\n  We show with three different problems in histopathology, retinal imaging, and\nelectron microscopy that with the proposed group CNNs, state-of-the-art\nperformance can be achieved, without the need for data augmentation by rotation\nand with increased performance compared to standard CNNs that do rely on\naugmentation."
  },
  {
    "id": "deep-convolutional-neural-networks-for-brain",
    "title": "Deep convolutional neural networks for brain image analysis on magnetic resonance imaging: a review",
    "abstract": "In recent years, deep convolutional neural networks (CNNs) have shown\nrecord-shattering performance in a variety of computer vision problems, such as\nvisual object recognition, detection and segmentation. These methods have also\nbeen utilised in medical image analysis domain for lesion segmentation,\nanatomical segmentation and classification. We present an extensive literature\nreview of CNN techniques applied in brain magnetic resonance imaging (MRI)\nanalysis, focusing on the architectures, pre-processing, data-preparation and\npost-processing strategies available in these works. The aim of this study is\nthree-fold. Our primary goal is to report how different CNN architectures have\nevolved, discuss state-of-the-art strategies, condense their results obtained\nusing public datasets and examine their pros and cons. Second, this paper is\nintended to be a detailed reference of the research activity in deep CNN for\nbrain MRI analysis. Finally, we present a perspective on the future of CNNs in\nwhich we hint some of the research directions in subsequent years."
  }
]